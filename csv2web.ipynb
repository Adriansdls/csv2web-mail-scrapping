{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import requests\n",
    "import re\n",
    "from requests.exceptions import ReadTimeout\n",
    "from requests.models import ReadTimeoutError\n",
    "from urllib.parse import urlsplit\n",
    "from collections import deque\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from googlesearch import search\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "# Your file\n",
    "filename = 'db.csv'\n",
    "df = pd.read_csv(filename, sep = \"\\t\", on_bad_lines='skip', encoding = 'utf-16')\n",
    "\n",
    "# Your column name.\n",
    "df[\"Full name\"] = df[\"Full name\"].apply(str.lower)\n",
    "df[\"Full name\"] = df[\"Full name\"].str.replace(\".\",\"\")\n",
    "\n",
    "# Three new colums for the emails\n",
    "df[\"M1\"] = \"\"\n",
    "df[\"M2\"] = \"\"\n",
    "df[\"M3\"] = \"\"\n",
    "\n",
    "# Reverse df. Erase if not needed.\n",
    "df = df.iloc[::-1]\n",
    "\n",
    "# The loop starts. \n",
    "for index, row in df.iterrows():\n",
    "  urls = []\n",
    "  original_url = urls\n",
    "  unscraped = deque([original_url])\n",
    "  scraped = set()\n",
    "  emails = set()\n",
    "\n",
    "  # Again, your row name and more stirngs if needed\n",
    "  author_search = row[\"Full name\"] + \" contact\"\n",
    "\n",
    "  author = row[\"Full name\"]\n",
    "\n",
    "  # Splitting for comparing results latter on.\n",
    "  author_sep = author.split()\n",
    "  \n",
    "  # Number of results\n",
    "  urls = list(search(author_search, tld=\"com\", num=10, stop=10, pause=2))\n",
    "  count = defaultdict(int)\n",
    "\n",
    "  # URL loop for first name search\n",
    "  for i in urls:\n",
    "    unscraped = deque([i])\n",
    "    while len(unscraped):\n",
    "        url = unscraped.popleft()  \n",
    "        scraped.add(url)\n",
    "\n",
    "        parts = urlsplit(url)\n",
    "            \n",
    "        base_url = \"{0.scheme}://{0.netloc}\".format(parts)\n",
    "        if '/' in parts.path:\n",
    "          path = url[:url.rfind('/')+1]\n",
    "        else:\n",
    "          path = url\n",
    "\n",
    "        print(\"Crawling URL %s\" % url)\n",
    "\n",
    "        # The try/except following need fine-tuning\n",
    "        try:\n",
    "            response = requests.get(url, allow_redirects=False, timeout = 5)\n",
    "        except HTTPError: # this need review. Pretty sure this is not good.\n",
    "          pass\n",
    "        except ReadTimeout:\n",
    "          pass\n",
    "        except (requests.exceptions.MissingSchema, requests.exceptions.ConnectionError):\n",
    "            continue\n",
    "\n",
    "        new_emails = set(re.findall(r\"[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.com\", response.text, re.I))\n",
    "        emails.update(new_emails) \n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "        for anchor in soup.find_all(\"a\"):\n",
    "          if \"href\" in anchor.attrs:\n",
    "            link = anchor.attrs[\"href\"]\n",
    "          else:\n",
    "            link = ''\n",
    "\n",
    "            if link.startswith('/'):\n",
    "                link = base_url + link\n",
    "            \n",
    "            elif not link.startswith('http'):\n",
    "                link = path + link\n",
    "\n",
    "            if not link.endswith(\".gz\"):\n",
    "              if not link in unscraped and not link in scraped:\n",
    "                  unscraped.append(link)\n",
    "\n",
    "  lst_emails = list(emails)\n",
    "\n",
    "  # Counting if substring in string\n",
    "  for i in author_sep:\n",
    "    for j in lst_emails:\n",
    "      if i in j:\n",
    "        count[j] = count[j] + len(i) \n",
    "  \n",
    "  # Selecting 3 top results\n",
    "  sorted_count = sorted(count, key=count.get, reverse = True)[:3]\n",
    "\n",
    "  # To the df\n",
    "  try:\n",
    "    df.loc[index,\"M1\"] = sorted_count[0]\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "  try:\n",
    "    df.loc[index, \"M2\"] = sorted_count[1]\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "  try:\n",
    "    df.loc[index, \"M3\"] = sorted_count[2]\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "# Final export of new .csv file\n",
    "df.to_csv('db_with_mails.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "76754b488622d1558836a355212b7aaf2b034ada1afcd05832ba767ca79fa7cd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
